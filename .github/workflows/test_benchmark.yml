name: Benchmark
on:
  pull_request:
    types: [opened, review_requested, ready_for_review, synchronize, unlocked]

concurrency:
  group: build-and-test-${{ github.ref }}
  cancel-in-progress: true

env:
  OSS_ACCESS_KEY_ID: ${{ secrets.OSS_ACCESS_KEY_ID }}
  OSS_ACCESS_KEY_SECRET: ${{ secrets.OSS_ACCESS_KEY_SECRET }}
  ONEFLOW_TIMEOUT_SECONDS: 90
  FLOW_VISION_SRC: flow_vision
  TEST_WITH_TORCH_IMG_TAG: registry.cn-beijing.aliyuncs.com/oneflow/test-with-pytorch-1.9.0-cuda10.2-cudnn7-runtime:9b7ac2af8823bf537a636f8589fd60f51d4af348
  ONEFLOW_SRC: oneflow-src
  ONEFLOW_REF: master

jobs:
  find-benchmark-cache:
    name: "Find benchmark cache"
    if: github.event.pull_request.draft == false && github.base_ref == 'main'
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.find-cache.outputs.matrix }}
    steps:
      - uses: actions/checkout@v2
      - name: Checkout Oneflow-Inc/oneflow
        uses: actions/checkout@v2
        with:
          repository: Oneflow-Inc/oneflow
          ref: ${{ env.ONEFLOW_REF }}
          path: ${{ env.ONEFLOW_SRC }}
      - uses: ./cache-complete/matrix/test
        name: find cache
        id: find-cache
        timeout-minutes: 5
        with:
          runner-labels: |
            self-hosted
            linux
            provision
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          devices: |
            cuda
          tests: |
            benchmark

  benchmark:
    name: Benchmark suite
    runs-on: ${{ matrix.runs-on }}
    if: github.event.pull_request.draft == false && github.base_ref == 'main'
    needs: [find-benchmark-cache]
    strategy:
      fail-fast: true
      max-parallel: 1
      matrix: ${{ fromJson(needs.find-benchmark-cache.outputs.matrix) }}
    env:
      ONEFLOW_SRC: .
      TEST_CONTAINER_NAME: "ci-benchmark"
      SSH_TANK_HOST: 192.168.1.13
      SSH_TANK_PATH: /tank
    steps:
      - name: Fix permissions
        if: ${{ contains(matrix.runs-on, 'self-hosted') }}
        run: |
          set -x
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf *
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf .pytest_cache
      - uses: actions/checkout@v2
        with:
          path: get-oneflow
      - name: Checkout Oneflow-Inc/oneflow
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          repository: ${{github.event.pull_request.head.repo.full_name}}
      - name: Remove container
        timeout-minutes: 45
        if: ${{ contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker rm -f ${{ env.TEST_CONTAINER_NAME }} || true
      - uses: ./get-oneflow/cache-complete
        name: Save cache if successful
        id: save-cache
        timeout-minutes: 5
        with:
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          entry: ${{ matrix.entry }}
          digest-type: ${{ matrix.digest-type }}
          mark-as-completed: ${{ contains(matrix.runs-on, 'self-hosted') && github.event.pull_request.head.repo.full_name == github.repository }}
      - name: Check digest cache result. If this step failed, usually it is caused by new commits pushed when this CI run is running.
        if: ${{ fromJSON(steps.save-cache.outputs.cache-hit) != matrix.cache-hit }}
        run: |
          echo "::error file=test.yml,line=204,col=10::steps.save-cache.outputs.cache-hit != matrix.cache-hit"
          exit 1
      - name: Download wheel and packed liboneflow
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        uses: ./get-oneflow/digest/download
        id: download-digest
        timeout-minutes: 10
        with:
          digest: ${{ steps.save-cache.outputs.build-digest }}
          entry: ${{ matrix.compute-platform }}
          ssh-tank-host: ${{ env.SSH_TANK_HOST }}
          ssh-tank-path: ${{ env.SSH_TANK_PATH }}
      - name: Set environment variables
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          set -x
          echo "ONEFLOW_TEST_CACHE_DIR=$HOME/ci-cache/test_cache" >> $GITHUB_ENV
          echo "ONEFLOW_WHEEL_PATH=${{ steps.download-digest.outputs.entry-dir }}/whl" >> $GITHUB_ENV
          echo "ONEFLOW_CPACK_PATH=${{ steps.download-digest.outputs.entry-dir }}/cpack" >> $GITHUB_ENV
      - name: Set environment variables (distributed)
        if: ${{ fromJson(matrix.is-distributed) }}
        run: |
          set -x
          EXTRA_DOCKER_ARGS+=" --network host "
          echo "EXTRA_DOCKER_ARGS=${EXTRA_DOCKER_ARGS}" >> $GITHUB_ENV
      - name: Enable ONEFLOW_TEST_VERBOSE
        if: ${{ contains(github.event.pull_request.labels.*.name, 'need-test-verbose') }}
        run: |
          EXTRA_DOCKER_ARGS+=" --env ONEFLOW_TEST_VERBOSE=1"
          echo "EXTRA_DOCKER_ARGS=${EXTRA_DOCKER_ARGS}" >> $GITHUB_ENV
      - name: Start container
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        working-directory: ${{ env.ONEFLOW_SRC }}
        run: |
          docker run -d --rm --privileged --shm-size=8g \
            --pids-limit -1 \
            --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \
            --runtime=nvidia \
            -v /dataset:/dataset:ro -v /model_zoo:/model_zoo:ro \
            -v ${ONEFLOW_WHEEL_PATH}:${ONEFLOW_WHEEL_PATH}:ro \
            -v $HOME/test-container-cache/dot-local:/root/.local \
            -v $HOME/test-container-cache/dot-cache:/root/.cache \
            -e ONEFLOW_WHEEL_PATH=${ONEFLOW_WHEEL_PATH} \
            -e ONEFLOW_CI=1 \
            -v $PWD:$PWD \
            -w $PWD \
            -v ${ONEFLOW_TEST_CACHE_DIR}:${ONEFLOW_TEST_CACHE_DIR} \
            -e ONEFLOW_TEST_CACHE_DIR=${ONEFLOW_TEST_CACHE_DIR} \
            -e ONEFLOW_TIMEOUT_SECONDS=${{ env.ONEFLOW_TIMEOUT_SECONDS }} \
            -e ONEFLOW_MLIR_ENABLE_ROUND_TRIP=1 \
            --name ${TEST_CONTAINER_NAME} \
            ${{ env.EXTRA_DOCKER_ARGS }} \
            ${{ env.TEST_WITH_TORCH_IMG_TAG }} \
            sleep 5400
      - name: Test container
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} ls
      - name: Install OneFlow
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          ls ${ONEFLOW_WHEEL_PATH}
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip install --pre oneflow -f https://staging.oneflow.info/branch/master/cu102
      - name: Checkout Oneflow-Inc/vision
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        uses: actions/checkout@v2
        with:
          repository: Oneflow-Inc/vision
          # please use a commit here
          ref: 0a291a00167143e64c7f7e5743ea93bf6a50a6b6
          path: ${{ env.FLOW_VISION_SRC}}
      - name: Install Flow Vision
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip install -e ${{ env.FLOW_VISION_SRC}}
      # start pytest benchmark
      - name: Benchmark alexnet
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_alexnet.py
          benchmark-id: 1-gpu-alexnet
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark convnext_tiny_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_convnext_tiny_224.py
          benchmark-id: 1-gpu-convnext_tiny_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark crossformer_tiny_patch4_group7_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_crossformer_tiny_patch4_group7_224.py
          benchmark-id: 1-gpu-crossformer_tiny_patch4_group7_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark cswin_tiny_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_cswin_tiny_224.py
          benchmark-id: 1-gpu-cswin_tiny_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark densent121
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_densent121.py
          benchmark-id: 1-gpu-densent121
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark ghostnet
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_ghostnet.py
          benchmark-id: 1-gpu-ghostnet
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark googlenet
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_googlenet.py
          benchmark-id: 1-gpu-googlenet
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark inception_v3
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_inception_v3.py
          benchmark-id: 1-gpu-inception_v3
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark mlp_mixer_b16_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_mlp_mixer_b16_224.py
          benchmark-id: 1-gpu-mlp_mixer_b16_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark mnasnet0_5
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_mnasnet0_5.py
          benchmark-id: 1-gpu-mnasnet0_5
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark mobilenet_v2
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_mobilenet_v2.py
          benchmark-id: 1-gpu-mobilenet_v2
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark mobilenet_v3
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_mobilenet_v3.py
          benchmark-id: 1-gpu-mobilenet_v3
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark poolformer_m36
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_poolformer_m36.py
          benchmark-id: 1-gpu-poolformer_m36
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark poolformer_m48
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_poolformer_m48.py
          benchmark-id: 1-gpu-poolformer_m48
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark poolformer_s12
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_poolformer_s12.py
          benchmark-id: 1-gpu-poolformer_s12
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark poolformer_s24
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_poolformer_s24.py
          benchmark-id: 1-gpu-poolformer_s24
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark poolformer_s36
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_poolformer_s36.py
          benchmark-id: 1-gpu-poolformer_s36
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark pvt_samll
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_pvt_samll.py
          benchmark-id: 1-gpu-pvt_samll
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark pvt_tiny
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_pvt_tiny.py
          benchmark-id: 1-gpu-pvt_tiny
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark res2net50_26w_4s
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_res2net50_26w_4s.py
          benchmark-id: 1-gpu-res2net50_26w_4s
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resmlp_12_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resmlp_12_224.py
          benchmark-id: 1-gpu-resmlp_12_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnest101
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnest101.py
          benchmark-id: 1-gpu-resnest101
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnest200
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnest200.py
          benchmark-id: 1-gpu-resnest200
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnest269
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnest269.py
          benchmark-id: 1-gpu-resnest269
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnest50
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnest50.py
          benchmark-id: 1-gpu-resnest50
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnet50
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnet50.py
          benchmark-id: 1-gpu-resnet50
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnext50_32x4d
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnext50_32x4d.py
          benchmark-id: 1-gpu-resnext50_32x4d
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark rexnet_lite_1_0
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_rexnet_lite_1_0.py
          benchmark-id: 1-gpu-rexnet_lite_1_0
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark rexnetv1_1_0
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_rexnetv1_1_0.py
          benchmark-id: 1-gpu-rexnetv1_1_0
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark shufflenet_v2_x0_5
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_shufflenet_v2_x0_5.py
          benchmark-id: 1-gpu-shufflenet_v2_x0_5
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark squeezenet1_0
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_squeezenet1_0.py
          benchmark-id: 1-gpu-squeezenet1_0
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark swin_base_patch4_window7_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_swin_base_patch4_window7_224.py
          benchmark-id: 1-gpu-swin_base_patch4_window7_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark swin_small_patch4_window7_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_swin_small_patch4_window7_224.py
          benchmark-id: 1-gpu-swin_small_patch4_window7_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark swin_tiny_patch4_window7_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_swin_tiny_patch4_window7_224.py
          benchmark-id: 1-gpu-swin_tiny_patch4_window7_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark uniformer_base_ls
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_uniformer_base_ls.py
          benchmark-id: 1-gpu-uniformer_base_ls
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark uniformer_base
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_uniformer_base.py
          benchmark-id: 1-gpu-uniformer_base
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark uniformer_small_plus
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_uniformer_small_plus.py
          benchmark-id: 1-gpu-uniformer_small_plus
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark uniformer_small
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_uniformer_small.py
          benchmark-id: 1-gpu-uniformer_small
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark vit_base_path16_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_vit_base_path16_224.py
          benchmark-id: 1-gpu-vit_base_path16_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark vit_small_path16_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_vit_small_path16_224.py
          benchmark-id: 1-gpu-vit_small_path16_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark vit-tiny_path16_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_vit-tiny_path16_224.py
          benchmark-id: 1-gpu-vit-tiny_path16_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark wide_resnet50_2
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: ./get-oneflow/pytest-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_wide_resnet50_2.py
          benchmark-id: 1-gpu-wide_resnet50_2
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      # end pytest benchmark
      - name: Print stacks in all core files
        timeout-minutes: 45
        if: ${{ failure() && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} bash ci/test/print_stack_in_all_dirs.sh || true
      - name: Remove automerge
        if: ${{ failure() && contains(matrix.runs-on, 'self-hosted') && cancelled() == false && contains(github.event.pull_request.labels.*.name, 'automerge') }}
        uses: actions/github-script@v4
        with:
          script: |
            github.issues.removeLabel({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'automerge'
            })
            github.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'CI failed when running job: ${{ matrix.entry }}. PR label automerge has been removed'
            })
      - name: Remove container
        timeout-minutes: 45
        if: ${{ always() && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker rm -f ${{ env.TEST_CONTAINER_NAME }} || true
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf *
